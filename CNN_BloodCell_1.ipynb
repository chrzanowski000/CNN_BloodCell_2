{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bbyayKmx5UkU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Whole notebook has been run on Paperspace Gradient,\n",
        "\n",
        "created by: MichaÅ‚ Chrzanowski 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ETIz7k-CqFE0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#modules not used in this notebook but may come in handy later\n",
        "\n",
        "# image processing of images\n",
        "from skimage.color import rgb2hsv, hsv2rgb\n",
        "def color_isolate(img):\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    img_hsv = rgb2hsv(img[:,:,:3])   \n",
        "    #MASK\n",
        "    mask_1 = img_hsv [:,:,0] > 200/360\n",
        "    mask_2 = img_hsv [:,:,0] < 280/360\n",
        "    saturation_1 = img_hsv [:,:,1] > 0.35\n",
        "    saturation_2 = img_hsv [:,:,1] < 0.70\n",
        "    \n",
        "    mask= mask_1*mask_1*saturation_1*saturation_2\n",
        "\n",
        "    image_filtered = np.dstack((img[:,:,0]*mask,\n",
        "                                img[:,:,1]*mask,\n",
        "                                img[:,:,2]*mask))\n",
        "\n",
        "    return image_filtered\n",
        "\n",
        "#used to preview images loaded in model\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()[0]\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTucYHtcAzU6",
        "outputId": "35553a2c-0d8e-49db-e6a2-6ca1a8038775",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.6 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2000.126\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4000.25\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 85\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "stepping\t: 3\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2000.126\n",
            "cache size\t: 39424 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4000.25\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         167G   39G  129G  23% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.7G   40K  5.7G   1% /dev/shm\n",
            "/dev/root       2.0G  1.1G  910M  54% /sbin/docker-init\n",
            "tmpfs           6.4G   40K  6.4G   1% /var/colab\n",
            "/dev/sda1       174G   40G  134G  23% /opt/bin/.nvidia\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "drive            15G  2.9G   13G  19% /content/drive\n",
            "MemTotal:       13297228 kB\n",
            "MemFree:         4615372 kB\n",
            "MemAvailable:    9664084 kB\n",
            "Buffers:          182932 kB\n",
            "Cached:          3990228 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1953052 kB\n",
            "Inactive:        6091196 kB\n",
            "Active(anon):        980 kB\n",
            "Inactive(anon):  2988796 kB\n",
            "Active(file):    1952072 kB\n",
            "Inactive(file):  3102400 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              1040 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:       3871024 kB\n",
            "Mapped:          1170480 kB\n",
            "Shmem:            179504 kB\n",
            "KReclaimable:     320164 kB\n",
            "Slab:             397204 kB\n",
            "SReclaimable:     320164 kB\n",
            "SUnreclaim:        77040 kB\n",
            "KernelStack:       22320 kB\n",
            "PageTables:        29080 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6648612 kB\n",
            "Committed_AS:    7675900 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       70892 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1416 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      453440 kB\n",
            "DirectMap2M:    13174784 kB\n",
            "DirectMap1G:     2097152 kB\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "speedtest-cli is already the newest version (2.0.0-1ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Retrieving speedtest.net configuration...\n",
            "Cannot retrieve speedtest configuration\n",
            "ERROR: HTTP Error 403: Forbidden\n"
          ]
        }
      ],
      "source": [
        "!lsb_release -a #Linux info\n",
        "!cat /proc/cpuinfo # CPU info\n",
        "!df -h #disc info\n",
        "!cat /proc/meminfo #RAM info\n",
        "!sudo apt install speedtest-cli #library for speedtesting\n",
        "!speedtest-cli # network speed test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkAHGr3iBN8t",
        "outputId": "efc3590c-0023-4cfa-940c-45feaff69ba1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Oct 26 17:19:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n",
            "| 41%   49C    P8    18W / 140W |  10395MiB / 16376MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi #GPU info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGTvVH2x63y2",
        "outputId": "2e1cb934-7c9c-4ba8-aa29-a07291d6a96f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "allocated CUDA memory:  0\n",
            "cached CUDA memory:  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.4)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.1.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.1)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.10.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import time\n",
        "import torch\n",
        "import torchvision  # torch package for vision related things\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "import torchvision.transforms as transforms  # Transformations and augmentations\n",
        "from torch import optim  # For optimizers\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import DataLoader #Dataloader module\n",
        "from torch.utils.data import Dataset # Dataset module\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils # for presentation of images in a grid\n",
        "from tqdm import tqdm #nice progress bar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage.io import imshow, imread\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Libraries for dataloader\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "import cv2\n",
        "#Checking whether GPU RAM is empty\n",
        "print('allocated CUDA memory: ',torch.cuda.memory_allocated())\n",
        "print('cached CUDA memory: ',torch.cuda.memory_cached())\n",
        "# I decided to use external plotting service Weights and biases. It provides easy live preview of model state and  allows convenient runs comparasion\n",
        "!pip install wandb\n",
        "!wandb login 33cccacf373072ab6a5edc82d4770dddf40d42fb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyper-parameters\n",
        "root = '/notebooks/dataset2-master/dataset2-master/images'\n",
        "in_channels = 3\n",
        "num_classes = 4\n",
        "learning_rate = 3e-4\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "weight_decay = 0.001\n",
        "\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d_%m_%Y\")\n",
        "# path to save pytorch models during training\n",
        "PATH = f'/notebooks/logs/model_{dt_string}.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os,time\n",
        "import cv2\n",
        "\n",
        "#img_dir='/notebooks/DATA'\n",
        "def verify_folder(folder_dir):\n",
        "    \n",
        "    def verify_image(img_file):\n",
        "         #test image\n",
        "         try:\n",
        "            v_image = Image.open(img_file)\n",
        "            v_image.verify()\n",
        "            return True;\n",
        "            #is valid\n",
        "            #print(\"valid file: \"+img_file)\n",
        "         except OSError:\n",
        "            return False;\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
        "                currentFile=os.path.join(root, file)\n",
        "                #print(currentFile)\n",
        "                #test image\n",
        "                if verify_image(currentFile):\n",
        "                    im = cv2.imread(currentFile)\n",
        "                    if im.shape[0] < 64 or im.shape[1] < 64:\n",
        "                        print('undersized file')\n",
        "                        os.remove(currentFile)\n",
        "                else:\n",
        "                    os.remove(currentFile)\n",
        "                    print(\"corrupt file\")\n",
        "                    \n",
        "            else:\n",
        "                os.remove(os.path.join(root, file))\n",
        "                print(\"unsupported file extension\")\n",
        "    print('DONE')\n",
        "                    \n",
        "verify_folder('/notebooks/dataset2-master/dataset2-master/images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MEoHkZFK7f_b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# DATALOADERS\n",
        "\n",
        "# Train Loader\n",
        "class BloodCell_train(Dataset):\n",
        "    def __init__(self, root ,transform):\n",
        "        self.labels = []\n",
        "        self.dirName_1 = join(root,'TRAIN/EOSINOPHIL') # path to training data with EOSINOPHILS cells\n",
        "        self.dirName_2 = join(root,'TRAIN/LYMPHOCYTE') # path to training data with LYMPHOCYTES cells\n",
        "        self.dirName_3 = join(root,'TRAIN/MONOCYTE') # path to training data with MONOCYTES cells\n",
        "        self.dirName_4 = join(root,'TRAIN/NEUTROPHIL') # path to training data with NEUTROPHILS cells\n",
        "        self.files_names = []\n",
        "        self.transform = transform\n",
        "        self.load_images()\n",
        "    \n",
        "\n",
        "    def load_images(self):\n",
        "\n",
        "        # create list of paths to images\n",
        "        files_1 = [join(self.dirName_1, f) for f in listdir(self.dirName_1) if isfile(join(self.dirName_1, f))]\n",
        "        files_2 = [join(self.dirName_2, f) for f in listdir(self.dirName_2) if isfile(join(self.dirName_2, f))]\n",
        "        files_3 = [join(self.dirName_3, f) for f in listdir(self.dirName_3) if isfile(join(self.dirName_3, f))]\n",
        "        files_4 = [join(self.dirName_4, f) for f in listdir(self.dirName_4) if isfile(join(self.dirName_4, f))]\n",
        "        \n",
        "\n",
        "        # connect lists of different cells in one big list\n",
        "        self.files_names = files_1 + files_2 + files_3 + files_4\n",
        "\n",
        "        # creating label list\n",
        "        for file_name in files_1:\n",
        "            self.labels.append(0)\n",
        "\n",
        "        for file_name in files_2:\n",
        "            self.labels.append(1)\n",
        "\n",
        "        for file_name in files_3:\n",
        "            self.labels.append(2)\n",
        "\n",
        "        for file_name in files_4:\n",
        "            self.labels.append(3)\n",
        "\n",
        "        del files_1\n",
        "        del files_2\n",
        "        del files_3\n",
        "        del files_4\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image=cv2.imread(self.files_names[index])\n",
        "        #image = color_isolate(image.astype(np.float32))\n",
        "        y_label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, y_label)\n",
        "\n",
        "# Test Loader\n",
        "class BloodCell_test(Dataset):\n",
        "    def __init__(self, root,transform):\n",
        "        self.labels = []\n",
        "        self.dirName_1 = join(root,'TEST/EOSINOPHIL') # path to test data with EOSINOPHILS cells\n",
        "        self.dirName_2 = join(root,'TEST/LYMPHOCYTE') # path to test data with LYMPHOCYTES cells\n",
        "        self.dirName_3 = join(root,'TEST/MONOCYTE') # path to test data with MONOCYTES cells\n",
        "        self.dirName_4 = join(root,'TEST/NEUTROPHIL') # path to test data with NEUTROPHILS cellsself.files_names = []\n",
        "        self.files_names = []\n",
        "        self.transform = transform\n",
        "        self.load_images()\n",
        "        \n",
        "\n",
        "    def load_images(self):\n",
        "\n",
        "        # create list of paths to images\n",
        "        files_1 = [join(self.dirName_1, f) for f in listdir(self.dirName_1) if isfile(join(self.dirName_1, f))]\n",
        "        files_2 = [join(self.dirName_2, f) for f in listdir(self.dirName_2) if isfile(join(self.dirName_2, f))]\n",
        "        files_3 = [join(self.dirName_3, f) for f in listdir(self.dirName_3) if isfile(join(self.dirName_3, f))]\n",
        "        files_4 = [join(self.dirName_4, f) for f in listdir(self.dirName_4) if isfile(join(self.dirName_4, f))]\n",
        "        \n",
        "        # connect lists of different cells in one big list\n",
        "        self.files_names = files_1 + files_2 + files_3 + files_4\n",
        "\n",
        "        # creating label list\n",
        "        for file_name in files_1:\n",
        "            self.labels.append(0)\n",
        "\n",
        "        for file_name in files_2:\n",
        "            self.labels.append(1)\n",
        "\n",
        "        for file_name in files_3:\n",
        "            self.labels.append(2)\n",
        "\n",
        "        for file_name in files_4:\n",
        "            self.labels.append(3)\n",
        "\n",
        "        del files_1\n",
        "        del files_2\n",
        "        del files_3\n",
        "        del files_4\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image=cv2.imread(self.files_names[index])\n",
        "        #image = color_isolate(image.astype(np.float32))\n",
        "        y_label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, y_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J9W5bQwo5UlD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|â–Œ         | 573/9957 [00:00<00:14, 669.73it/s]"
          ]
        }
      ],
      "source": [
        "# for calcualting mean and std od dataset\n",
        "def get_mean_and_std(dataset):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "    for data, _ in tqdm(dataset):\n",
        "        # Mean over batch, height and width, but not over the channels\n",
        "        channels_sum += torch.mean(data, dim=[1,2])\n",
        "        channels_squared_sum += torch.mean(data**2, dim=[1,2])\n",
        "        num_batches += 1\n",
        "\n",
        "    mean = channels_sum / num_batches\n",
        "\n",
        "    # std = sqrt(E[X^2] - (E[X])^2)\n",
        "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "Norm_transform = transforms.Compose([transforms.ToTensor()])\n",
        "Norm_dataset_train = BloodCell_train(root=root,transform=Norm_transform)\n",
        "Norm_dataset_test = BloodCell_test(root=root,transform=Norm_transform)\n",
        "\n",
        "mean_train, std_train = get_mean_and_std(Norm_dataset_train)\n",
        "mean_test, std_test = get_mean_and_std(Norm_dataset_test)\n",
        "\n",
        "Norm_dataloader_train = torch.utils.data.DataLoader(dataset=Norm_dataset_train, batch_size=64, shuffle=True, num_workers=2)\n",
        "Norm_loaded_images=next(iter(Norm_dataloader_train))[0]\n",
        "loaded_images_array = np.transpose(vutils.make_grid(Norm_loaded_images, padding=2, normalize=True).cpu().detach().numpy(),(1,2,0))\n",
        "plt.imshow(loaded_images_array)\n",
        "\n",
        "del Norm_dataloader_train, Norm_dataset_train, Norm_dataset_test\n",
        "print(f'TRAIN dataset ,mean: {mean_train.tolist()}, std: {std_train.tolist()}')\n",
        "print(f'TEST dataset ,mean: {mean_test.tolist()}, std: {std_test.tolist()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrgL0wmx5Uk6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# model creation\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=9,\n",
        "            kernel_size=(11, 11),\n",
        "            stride=(1, 1),\n",
        "            padding=(1,1),\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=9,\n",
        "            out_channels=27,\n",
        "            kernel_size=(7, 7),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=27,\n",
        "            out_channels=54,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "        self.conv4 = nn.Conv2d(\n",
        "            in_channels=54,\n",
        "            out_channels=54,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=(1, 1),\n",
        "            padding=(1,1),\n",
        "        )\n",
        "        self.conv5 = nn.Conv2d(\n",
        "            in_channels=54,\n",
        "            out_channels=108,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "        self.conv6 = nn.Conv2d(\n",
        "            in_channels=108,\n",
        "            out_channels=108,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "        self.conv7 = nn.Conv2d(\n",
        "            in_channels=108,\n",
        "            out_channels=108,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "        self.conv8 = nn.Conv2d(\n",
        "            in_channels=108,\n",
        "            out_channels=108,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(0,0),\n",
        "        )\n",
        "\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=(5, 5), stride=(5, 5))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.drop1d = nn.Dropout(0.5)\n",
        "        self.BatchNorm2d1 = nn.BatchNorm2d(num_features=9)\n",
        "        self.BatchNorm2d3 = nn.BatchNorm2d(num_features=54)\n",
        "        self.fc1 = nn.Linear(41472, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 5000)\n",
        "        self.fc3 = nn.Linear(5000, 5000)\n",
        "        self.fc4 = nn.Linear(5000, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.BatchNorm2d1(self.conv1(x)))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.BatchNorm2d3(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool5(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv8(x))\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        #x = self.drop1d(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.drop1d(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.drop1d(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA56kxVRIL6P",
        "outputId": "f4864a1b-31cf-4464-b74e-423d215dc97e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial shape 320 240\n",
            "after 1st conv 312.0 232.0\n",
            "after 2nd conv 306.0 226.0\n",
            "after 3rd conv 302.0 222.0\n",
            "after 4th conv 300.0 220.0\n",
            "after 1st pooling 60.0 44.0\n",
            "after 5th conv 56.0 40.0\n",
            "after 6th conv 54.0 38.0\n",
            "after 7th conv 52.0 36.0\n",
            "after 2nd pooling 26.0 18.0\n",
            "after 8th conv 24.0 16.0\n",
            "Size of end output 41472.0\n"
          ]
        }
      ],
      "source": [
        "# calculate outputs shape of layers\n",
        "W=320\n",
        "H=240\n",
        "print('initial shape',W,H)\n",
        "#1st conv\n",
        "W=(W-11+2*1)/1+1\n",
        "H=(H-11+2*1)/1+1\n",
        "print('after 1st conv',W,H)\n",
        "# 2st conv\n",
        "W=(W-7+2*0)/1+1\n",
        "H=(H-7+2*0)/1+1\n",
        "print('after 2nd conv',W,H)\n",
        "# 3nd conv\n",
        "W=(W-5+2*0)/1+1\n",
        "H=(H-5+2*0)/1+1\n",
        "print('after 3rd conv',W,H)\n",
        "# 4nd conv#\n",
        "W=(W-5+2*1)/1+1\n",
        "H=(H-5+2*1)/1+1\n",
        "print('after 4th conv',W,H)\n",
        "# pool5\n",
        "W=W/5\n",
        "H=H/5\n",
        "print('after 1st pooling',W,H)\n",
        "# 5nd conv\n",
        "W=(W-5+2*0)/1+1\n",
        "H=(H-5+2*0)/1+1\n",
        "print('after 5th conv',W,H)\n",
        "# 6nd conv\n",
        "W=(W-3+2*0)/1+1\n",
        "H=(H-3+2*0)/1+1\n",
        "print('after 6th conv',W,H)\n",
        "# 7nd conv\n",
        "W=(W-3+2*0)/1+1\n",
        "H=(H-3+2*0)/1+1\n",
        "print('after 7th conv',W,H)\n",
        "# pool2\n",
        "W=W/2\n",
        "H=H/2\n",
        "print('after 2nd pooling',W,H)\n",
        "# 8nd conv\n",
        "W=(W-3+2*0)/1+1\n",
        "H=(H-3+2*0)/1+1\n",
        "print('after 8th conv',W,H)\n",
        "\n",
        "number_out_channels = 108\n",
        "print('Size of end output',H*W*number_out_channels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "8J5dzrruHsBe",
        "outputId": "9296df64-69b5-4f49-f01e-3867810b649b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrzanowski000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/notebooks/wandb/run-20221026_173412-360v5py7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/chrzanowski000/CNN_BloodCell_3/runs/360v5py7\" target=\"_blank\">iconic-terrain-2</a></strong> to <a href=\"https://wandb.ai/chrzanowski000/CNN_BloodCell_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2497/2497 [00:00<00:00, 662314.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2483/2483 [00:00<00:00, 573989.02it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2478/2478 [00:00<00:00, 616824.05it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2499/2499 [00:00<00:00, 578229.48it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 623/623 [00:00<00:00, 545180.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [00:00<00:00, 515250.34it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [00:00<00:00, 368234.00it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624/624 [00:00<00:00, 395892.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trained parameters in model: 257 980 958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'using: {device}')\n",
        "torch.backends.cudnn.benchmark = True #let cudnn chose most efficient way of calculating convulsions\n",
        "\n",
        "#Connenting to wandb project to log data there\n",
        "import wandb\n",
        "#wandb.init(mode=\"disabled\")\n",
        "wandb.init(project=\"CNN_BloodCell_3\")\n",
        "\n",
        "# Initialize network\n",
        "model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
        "model.to(device) \n",
        "\n",
        "# define transformations for datasets, stds and means have been calculated before with cell above on dataloaders without normalization\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean_train,std_train)])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean_test, std_test)])\n",
        "\n",
        "#initialize datasets and dataloaders\n",
        "dataset_train = BloodCell_train(root=root, transform=transform_train)\n",
        "dataset_test = BloodCell_test(root=root, transform=transform_test)\n",
        "\n",
        "# set shuffle = True to randomize order\n",
        "# To avoid blocking computation code with data loading we set num_workers = 2\n",
        "# pin_memory = True will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.\n",
        "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Calculate number of learnable parameters in our model\n",
        "mode_par_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of trained parameters in model: \" + '{:,}'.format(mode_par_num).replace(',', ' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBZBXfxrjtXE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    # Add more metrics as F1 and F2, ACC, PREC\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct / num_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sA7HZIVfBmWa",
        "outputId": "898dccca-827a-4283-802b-85b6b9a0fb9f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch nr 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312/312 [00:20<00:00, 15.49it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:05<00:00, 14.89it/s]\n",
            "  0%|          | 0/100 [01:47<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "Can't pickle local object 'TorchHistory.add_log_parameters_hook.<locals>.<lambda>'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3@student.uw.edu.pl/My Drive/Files/Programming/Machine Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3%40student.uw.edu.pl/My%20Drive/Files/Programming/Machine%20Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m test_acc \u001b[39m=\u001b[39m check_accuracy(test_loader, model)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3%40student.uw.edu.pl/My%20Drive/Files/Programming/Machine%20Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m test_acc \u001b[39m>\u001b[39m test_acc_best:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3%40student.uw.edu.pl/My%20Drive/Files/Programming/Machine%20Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model, PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3%40student.uw.edu.pl/My%20Drive/Files/Programming/Machine%20Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     test_acc_best \u001b[39m=\u001b[39m test_acc\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michalchrzanowski/Library/CloudStorage/GoogleDrive-mp.chrzanowsk3%40student.uw.edu.pl/My%20Drive/Files/Programming/Machine%20Learning/CNN_BloodCell/CNN_BloodCell_1.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     train_acc_best \u001b[39m=\u001b[39m train_acc\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    380\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py:589\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    587\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    588\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 589\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    590\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    591\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'TorchHistory.add_log_parameters_hook.<locals>.<lambda>'"
          ]
        }
      ],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "start_time = time.time()\n",
        "# Optional\n",
        "wandb.watch(model, criterion, log=\"all\", log_freq=400)\n",
        "train_acc_best = 0\n",
        "test_acc_best = 0\n",
        "# Train Network\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    print(f'epoch nr {epoch}')\n",
        "    for data, targets in train_loader:\n",
        "        # Get data to device\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # setting parameteres gradients to None, takes less memory than setting to '0'\n",
        "        for param in model.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "        # forward and loss calculation\n",
        "        scores=model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        # log loss to wandb\n",
        "        wandb.log({\"loss\": loss})\n",
        "\n",
        "        # backward\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Calculate Accuracy and save model if test acc has increased\n",
        "    train_acc = check_accuracy(train_loader, model)*100\n",
        "    test_acc = check_accuracy(test_loader, model)*100\n",
        "    if test_acc > test_acc_best:\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        test_acc_best = test_acc\n",
        "        train_acc_best = train_acc\n",
        "        print(f'model saved: train_acc:{train_acc_best}, test_acc:{test_acc_best}')\n",
        "\n",
        "    # log train_acc and test_acc to wandb\n",
        "    wandb.log({\"train_acc\": train_acc})\n",
        "    wandb.log({\"test_acc\": test_acc})\n",
        "    print(f\"Accuracy on training set: {train_acc:.2f}\")    \n",
        "    print(f\"Accuracy on test set: {test_acc:.2f}\")\n",
        "\n",
        "    #print('cached CUDA memory: ',torch.cuda.memory_cached())\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(f\"Highest accuracy on training set: {train_acc_best}\")\n",
        "print(f\"Highest accuracy on test set: {test_acc_best}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
